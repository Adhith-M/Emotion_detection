{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2024-12-10T17:49:28.769608Z",
     "iopub.status.busy": "2024-12-10T17:49:28.769345Z",
     "iopub.status.idle": "2024-12-10T17:49:31.592203Z",
     "shell.execute_reply": "2024-12-10T17:49:31.591244Z",
     "shell.execute_reply.started": "2024-12-10T17:49:28.769583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, Activation\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:49:31.593719Z",
     "iopub.status.busy": "2024-12-10T17:49:31.593487Z",
     "iopub.status.idle": "2024-12-10T17:49:34.794428Z",
     "shell.execute_reply": "2024-12-10T17:49:34.793394Z",
     "shell.execute_reply.started": "2024-12-10T17:49:31.593694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"fer2013\\fer2013\\fer2013.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:49:34.796841Z",
     "iopub.status.busy": "2024-12-10T17:49:34.796428Z",
     "iopub.status.idle": "2024-12-10T17:49:34.803904Z",
     "shell.execute_reply": "2024-12-10T17:49:34.803165Z",
     "shell.execute_reply.started": "2024-12-10T17:49:34.796796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.emotion.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:49:34.807333Z",
     "iopub.status.busy": "2024-12-10T17:49:34.807006Z",
     "iopub.status.idle": "2024-12-10T17:49:34.817168Z",
     "shell.execute_reply": "2024-12-10T17:49:34.816193Z",
     "shell.execute_reply.started": "2024-12-10T17:49:34.807291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "emotion_label_to_text = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:49:34.820572Z",
     "iopub.status.busy": "2024-12-10T17:49:34.820220Z",
     "iopub.status.idle": "2024-12-10T17:49:34.835232Z",
     "shell.execute_reply": "2024-12-10T17:49:34.834198Z",
     "shell.execute_reply.started": "2024-12-10T17:49:34.820532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:49:34.837248Z",
     "iopub.status.busy": "2024-12-10T17:49:34.836919Z",
     "iopub.status.idle": "2024-12-10T17:49:34.987192Z",
     "shell.execute_reply": "2024-12-10T17:49:34.986338Z",
     "shell.execute_reply.started": "2024-12-10T17:49:34.837217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(df.emotion)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`So majority classes belongs to 3:Happy, 4:Sad and 6:Neutral nd So we are using these three classes only.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:49:34.988753Z",
     "iopub.status.busy": "2024-12-10T17:49:34.988501Z",
     "iopub.status.idle": "2024-12-10T17:49:34.994974Z",
     "shell.execute_reply": "2024-12-10T17:49:34.993936Z",
     "shell.execute_reply.started": "2024-12-10T17:49:34.988726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "math.sqrt(len(df.pixels[0].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:49:34.996661Z",
     "iopub.status.busy": "2024-12-10T17:49:34.996410Z",
     "iopub.status.idle": "2024-12-10T17:49:43.094977Z",
     "shell.execute_reply": "2024-12-10T17:49:43.093832Z",
     "shell.execute_reply.started": "2024-12-10T17:49:34.996634Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig = pyplot.figure(1, (14, 14))\n",
    "\n",
    "k = 0\n",
    "for label in sorted(df.emotion.unique()):\n",
    "    for j in range(7):\n",
    "        px = df[df.emotion==label].pixels.iloc[k]\n",
    "        px = np.array(px.split(' ')).reshape(48, 48).astype('float32')\n",
    "\n",
    "        k += 1\n",
    "        ax = pyplot.subplot(7, 7, k)\n",
    "        ax.imshow(px, cmap='gray')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(emotion_label_to_text[label])\n",
    "        pyplot.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:49:43.096649Z",
     "iopub.status.busy": "2024-12-10T17:49:43.096366Z",
     "iopub.status.idle": "2024-12-10T17:49:43.100591Z",
     "shell.execute_reply": "2024-12-10T17:49:43.099634Z",
     "shell.execute_reply.started": "2024-12-10T17:49:43.096618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "INTERESTED_LABELS = [3, 4, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:49:43.102345Z",
     "iopub.status.busy": "2024-12-10T17:49:43.102031Z",
     "iopub.status.idle": "2024-12-10T17:49:43.120994Z",
     "shell.execute_reply": "2024-12-10T17:49:43.120190Z",
     "shell.execute_reply.started": "2024-12-10T17:49:43.102315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df[df.emotion.isin(INTERESTED_LABELS)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Now I will make the data compatible for neural networks.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:49:43.122688Z",
     "iopub.status.busy": "2024-12-10T17:49:43.122414Z",
     "iopub.status.idle": "2024-12-10T17:50:12.098025Z",
     "shell.execute_reply": "2024-12-10T17:50:12.097163Z",
     "shell.execute_reply.started": "2024-12-10T17:49:43.122659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img_array = df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\n",
    "img_array = np.stack(img_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:50:12.100144Z",
     "iopub.status.busy": "2024-12-10T17:50:12.099733Z",
     "iopub.status.idle": "2024-12-10T17:50:12.106820Z",
     "shell.execute_reply": "2024-12-10T17:50:12.105834Z",
     "shell.execute_reply.started": "2024-12-10T17:50:12.100096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:50:12.109139Z",
     "iopub.status.busy": "2024-12-10T17:50:12.108748Z",
     "iopub.status.idle": "2024-12-10T17:50:12.126810Z",
     "shell.execute_reply": "2024-12-10T17:50:12.125669Z",
     "shell.execute_reply.started": "2024-12-10T17:50:12.109094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "\n",
    "le = LabelEncoder()\n",
    "img_labels = le.fit_transform(df.emotion)\n",
    "img_labels = to_categorical(img_labels)\n",
    "img_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:50:12.128896Z",
     "iopub.status.busy": "2024-12-10T17:50:12.128586Z",
     "iopub.status.idle": "2024-12-10T17:50:12.139784Z",
     "shell.execute_reply": "2024-12-10T17:50:12.138497Z",
     "shell.execute_reply.started": "2024-12-10T17:50:12.128851Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Splitting the data into training and validation set.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:50:12.141765Z",
     "iopub.status.busy": "2024-12-10T17:50:12.141420Z",
     "iopub.status.idle": "2024-12-10T17:50:12.324760Z",
     "shell.execute_reply": "2024-12-10T17:50:12.323704Z",
     "shell.execute_reply.started": "2024-12-10T17:50:12.141727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(img_array, img_labels,\n",
    "                                                    shuffle=True, stratify=img_labels,\n",
    "                                                    test_size=0.1, random_state=42)\n",
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:50:12.326491Z",
     "iopub.status.busy": "2024-12-10T17:50:12.326211Z",
     "iopub.status.idle": "2024-12-10T17:50:12.332650Z",
     "shell.execute_reply": "2024-12-10T17:50:12.331690Z",
     "shell.execute_reply.started": "2024-12-10T17:50:12.326459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "del df\n",
    "del img_array\n",
    "del img_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:50:12.334131Z",
     "iopub.status.busy": "2024-12-10T17:50:12.333831Z",
     "iopub.status.idle": "2024-12-10T17:50:12.345026Z",
     "shell.execute_reply": "2024-12-10T17:50:12.343994Z",
     "shell.execute_reply.started": "2024-12-10T17:50:12.334100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img_width = X_train.shape[1]\n",
    "img_height = X_train.shape[2]\n",
    "img_depth = X_train.shape[3]\n",
    "num_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:50:12.346784Z",
     "iopub.status.busy": "2024-12-10T17:50:12.346467Z",
     "iopub.status.idle": "2024-12-10T17:50:12.416557Z",
     "shell.execute_reply": "2024-12-10T17:50:12.415456Z",
     "shell.execute_reply.started": "2024-12-10T17:50:12.346731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Normalizing results, as neural networks are very sensitive to unnormalized data.\n",
    "X_train = X_train / 255.\n",
    "X_valid = X_valid / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:50:12.418427Z",
     "iopub.status.busy": "2024-12-10T17:50:12.418157Z",
     "iopub.status.idle": "2024-12-10T17:50:12.436220Z",
     "shell.execute_reply": "2024-12-10T17:50:12.434979Z",
     "shell.execute_reply.started": "2024-12-10T17:50:12.418397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_net(optim):\n",
    "    \"\"\"\n",
    "    This is a Deep Convolutional Neural Network (DCNN). For generalization purpose I used dropouts in regular intervals.\n",
    "    I used `ELU` as the activation because it avoids dying relu problem but also performed well as compared to LeakyRelu\n",
    "    atleast in this case. `he_normal` kernel initializer is used as it suits ELU. BatchNormalization is also used for better\n",
    "    results.\n",
    "    \"\"\"\n",
    "    net = Sequential(name='DCNN')\n",
    "\n",
    "    net.add(\n",
    "        Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=(5,5),\n",
    "            input_shape=(img_width, img_height, img_depth),\n",
    "            activation='elu',\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            name='conv2d_1'\n",
    "        )\n",
    "    )\n",
    "    net.add(BatchNormalization(name='batchnorm_1'))\n",
    "    net.add(\n",
    "        Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=(5,5),\n",
    "            activation='elu',\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            name='conv2d_2'\n",
    "        )\n",
    "    )\n",
    "    net.add(BatchNormalization(name='batchnorm_2'))\n",
    "    \n",
    "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_1'))\n",
    "    net.add(Dropout(0.4, name='dropout_1'))\n",
    "\n",
    "    net.add(\n",
    "        Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=(3,3),\n",
    "            activation='elu',\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            name='conv2d_3'\n",
    "        )\n",
    "    )\n",
    "    net.add(BatchNormalization(name='batchnorm_3'))\n",
    "    net.add(\n",
    "        Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=(3,3),\n",
    "            activation='elu',\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            name='conv2d_4'\n",
    "        )\n",
    "    )\n",
    "    net.add(BatchNormalization(name='batchnorm_4'))\n",
    "    \n",
    "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_2'))\n",
    "    net.add(Dropout(0.4, name='dropout_2'))\n",
    "\n",
    "    net.add(\n",
    "        Conv2D(\n",
    "            filters=256,\n",
    "            kernel_size=(3,3),\n",
    "            activation='elu',\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            name='conv2d_5'\n",
    "        )\n",
    "    )\n",
    "    net.add(BatchNormalization(name='batchnorm_5'))\n",
    "    net.add(\n",
    "        Conv2D(\n",
    "            filters=256,\n",
    "            kernel_size=(3,3),\n",
    "            activation='elu',\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            name='conv2d_6'\n",
    "        )\n",
    "    )\n",
    "    net.add(BatchNormalization(name='batchnorm_6'))\n",
    "    \n",
    "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_3'))\n",
    "    net.add(Dropout(0.5, name='dropout_3'))\n",
    "\n",
    "    net.add(Flatten(name='flatten'))\n",
    "        \n",
    "    net.add(\n",
    "        Dense(\n",
    "            128,\n",
    "            activation='elu',\n",
    "            kernel_initializer='he_normal',\n",
    "            name='dense_1'\n",
    "        )\n",
    "    )\n",
    "    net.add(BatchNormalization(name='batchnorm_7'))\n",
    "    \n",
    "    net.add(Dropout(0.6, name='dropout_4'))\n",
    "    \n",
    "    net.add(\n",
    "        Dense(\n",
    "            num_classes,\n",
    "            activation='softmax',\n",
    "            name='out_layer'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    net.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optim,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    net.summary()\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:50:12.438096Z",
     "iopub.status.busy": "2024-12-10T17:50:12.437790Z",
     "iopub.status.idle": "2024-12-10T17:50:12.455731Z",
     "shell.execute_reply": "2024-12-10T17:50:12.454690Z",
     "shell.execute_reply.started": "2024-12-10T17:50:12.438036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0.00005,\n",
    "    patience=11,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=7,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    early_stopping,\n",
    "    lr_scheduler,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:50:12.457184Z",
     "iopub.status.busy": "2024-12-10T17:50:12.456899Z",
     "iopub.status.idle": "2024-12-10T17:50:12.513078Z",
     "shell.execute_reply": "2024-12-10T17:50:12.512068Z",
     "shell.execute_reply.started": "2024-12-10T17:50:12.457153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# As the data in hand is less as compared to the task so ImageDataGenerator is good to go.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "train_datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:50:12.515201Z",
     "iopub.status.busy": "2024-12-10T17:50:12.514801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32  # batch size of 32 performs the best.\n",
    "epochs = 100\n",
    "optims = [\n",
    "    optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Nadam'),\n",
    "    optimizers.Adam(0.001),\n",
    "]\n",
    "\n",
    "# Build the model\n",
    "model = build_net(optims[1])\n",
    "\n",
    "# Fit the model (no `use_multiprocessing` argument)\n",
    "history = model.fit(\n",
    "    train_datagen.flow(X_train, y_train, batch_size=batch_size),  # Data generator\n",
    "    validation_data=(X_valid, y_valid),  # Validation data\n",
    "    steps_per_epoch=len(X_train) // batch_size,  # Integer division for steps per epoch\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks  # You can still include your callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the architecture in JSON format\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "    \n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # type: ignore\n",
    "\n",
    "# Set the context and style (if needed)\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create a figure with specified size\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Subplot for accuracy\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "sns.lineplot(x=history.epoch, y=history.history['accuracy'], label='Train')\n",
    "sns.lineplot(x=history.epoch, y=history.history['val_accuracy'], label='Validation')\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "# Subplot for loss\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "sns.lineplot(x=history.epoch, y=history.history['loss'], label='Train')\n",
    "sns.lineplot(x=history.epoch, y=history.history['val_loss'], label='Validation')\n",
    "ax2.set_title('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust layout and save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('epoch_history_dcnn.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The epochs history shows that accuracy gradually increases and achieved +83% accuracy on both training and validation set, but at the end the model starts overfitting training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Assuming 'history.history' is a dictionary with training metrics\n",
    "df_accu = pd.DataFrame({'train': history.history['accuracy'], 'valid': history.history['val_accuracy']})\n",
    "df_loss = pd.DataFrame({'train': history.history['loss'], 'valid': history.history['val_loss']})\n",
    "\n",
    "fig = pyplot.figure(0, (14, 4))\n",
    "\n",
    "# Accuracy violin plot\n",
    "ax = pyplot.subplot(1, 2, 1)\n",
    "sns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_accu), cut=0)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.tight_layout()\n",
    "\n",
    "# Loss violin plot\n",
    "ax = pyplot.subplot(1, 2, 2)\n",
    "sns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_loss), cut=0)\n",
    "pyplot.title('Loss')\n",
    "pyplot.tight_layout()\n",
    "\n",
    "pyplot.savefig('performance_dist.png')\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict the class labels for the validation set\n",
    "yhat_valid = np.argmax(model.predict(X_valid), axis=1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(y_valid, axis=1), yhat_valid)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = pyplot.subplots(figsize=(7, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=np.arange(cm.shape[1]), yticklabels=np.arange(cm.shape[0]))\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Confusion Matrix')\n",
    "pyplot.savefig(\"confusion_matrix_dcnn.png\")\n",
    "pyplot.show()\n",
    "\n",
    "# Calculate and display the total number of wrong predictions\n",
    "wrong_predictions = np.sum(np.argmax(y_valid, axis=1) != yhat_valid)\n",
    "print(f\"Total wrong validation predictions: {wrong_predictions}\\n\\n\")\n",
    "\n",
    "# Display the classification report\n",
    "print(classification_report(np.argmax(y_valid, axis=1), yhat_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix clearly shows that our model is doing good job on the class `happy` but it's performance is low on other two classes. One of the reason for this could be the fact that these two classes have less data. But when I looked at the images I found some images from these two classes are even hard for a human to tell whether the person is sad or neutral. Facial expression depends on individual as well. Some person's neutral face looks like sad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mapper = {\n",
    "    0: \"happy\",\n",
    "    1: \"sad\",\n",
    "    2: \"neutral\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(2)\n",
    "\n",
    "# Randomly sample images for sad and neutral classes\n",
    "random_sad_imgs = np.random.choice(np.where(y_valid[:, 1] == 1)[0], size=9)\n",
    "random_neutral_imgs = np.random.choice(np.where(y_valid[:, 2] == 1)[0], size=9)\n",
    "\n",
    "# Set the figure size\n",
    "fig = pyplot.figure(1, (18, 4))\n",
    "\n",
    "# Loop to plot images and their predictions\n",
    "for i, (sadidx, neuidx) in enumerate(zip(random_sad_imgs, random_neutral_imgs)):\n",
    "    # Plot for sad images\n",
    "    ax = pyplot.subplot(2, 9, i + 1)\n",
    "    sample_img = X_valid[sadidx, :, :, 0]  # Select the sad image\n",
    "    ax.imshow(sample_img, cmap='gray')\n",
    "    ax.set_xticks([])  \n",
    "    ax.set_yticks([])  \n",
    "    sad_pred = np.argmax(model.predict(sample_img.reshape(1, 48, 48, 1)), axis=-1)  # Predict class\n",
    "    ax.set_title(f\"true:sad, pred:{mapper[sad_pred[0]]}\")\n",
    "\n",
    "    ax = pyplot.subplot(2, 9, i + 10)\n",
    "    sample_img = X_valid[neuidx, :, :, 0]  # Select the neutral image\n",
    "    ax.imshow(sample_img, cmap='gray')\n",
    "    ax.set_xticks([])  # Hide x-axis ticks\n",
    "    ax.set_yticks([])  # Hide y-axis ticks\n",
    "    neutral_pred = np.argmax(model.predict(sample_img.reshape(1, 48, 48, 1)), axis=-1)  # Predict class\n",
    "    ax.set_title(f\"t:neut, p:{mapper[neutral_pred[0]]}\")\n",
    "\n",
    "# Adjust layout for better spacing between plots\n",
    "pyplot.tight_layout()\n",
    "pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 64677,
     "sourceId": 127160,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29867,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
